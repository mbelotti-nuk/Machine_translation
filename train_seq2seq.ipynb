{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architecture.seq2seq_module import Seq2Seq, seq2seq_encoder, seq2seq_decoder\n",
    "from tokenization.tokenizer_module import language_set, spec_tokens, translation_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "seed = 1996\n",
    "torch.manual_seed(seed)\n",
    "torch.mps.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_path):\n",
    "    txt = None\n",
    "    with open(file_path) as f:\n",
    "        txt = f.read().split(\"\\n\")\n",
    "    txt = [t.strip() for t in txt]\n",
    "    temp = []\n",
    "    for t in txt:\n",
    "        if len(t) > 0:\n",
    "            temp.append(t)\n",
    "    txt = temp\n",
    "    return np.array(txt)\n",
    "\n",
    "\n",
    "def split_train_test_val(it_data, fr_data, perc_test = 0.1, n_val_cases = 100):\n",
    "    assert len(it_data) > n_val_cases\n",
    "    assert len(it_data) - n_val_cases > 0\n",
    "    n_train_test, n = len(it_data) - n_val_cases, len(it_data)\n",
    "    n_test = int(n_train_test * (1-perc_test))\n",
    "    indices = random.sample(range(len(it_data)), len(it_data))\n",
    "    train_indices, test_indices, val_indices  = indices[:n_test], indices[n_test:-n_val_cases], indices[n-n_val_cases:]\n",
    "    return (language_set(source=it_data[train_indices], target=fr_data[train_indices]), \n",
    "            language_set(source=it_data[test_indices], target=fr_data[test_indices]),\n",
    "            language_set(source=it_data[val_indices], target=fr_data[val_indices]) )\n",
    "\n",
    "\n",
    "root = \"data/it_fr/\"\n",
    "\n",
    "it_data = get_data(os.path.join(root, \"Tatoeba.fr-it.it\"))\n",
    "fr_data = get_data(os.path.join(root, \"Tatoeba.fr-it.fr\"))\n",
    "\n",
    "train_data, test_data, val_data = split_train_test_val(it_data, fr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class translation_dataset(Dataset):\n",
    "    def __init__(self, data:language_set):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data.source)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.source[idx], self.data.target[idx]\n",
    "\n",
    "train_set = translation_dataset(train_data)\n",
    "test_set = translation_dataset(test_data)\n",
    "val_set = translation_dataset(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10_000\n",
    "MAX_SEQUENCE_LEN = 20\n",
    "\n",
    "tokenizer = translation_tokenizer(VOCAB_SIZE, MAX_SEQUENCE_LEN)\n",
    "tokenizer.set_tokenizers(language_set(source=[*train_data.source, *test_data.source], target=[*train_data.target, *test_data.target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_tokenizer(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.src_wrap.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for k in range(0,5):\n",
    "    i=random.randint(0, len(train_data.source))\n",
    "    print(tokenizer.src_wrap(train_data.source[i]).tokens(), \n",
    "          tokenizer.trg_wrap(train_data.target[i]).tokens())\n",
    "    \n",
    "    print(tokenizer.src_wrap.encode(train_data.source[i]), \n",
    "          tokenizer.trg_wrap.encode(train_data.target[i]))\n",
    "    \n",
    "    print(tokenizer.src_wrap.decode(tokenizer.src_wrap.encode(train_data.source[i])), \n",
    "          tokenizer.trg_wrap.decode(tokenizer.trg_wrap.encode(train_data.target[i])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.src_wrap(train_data.source[i]).word_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\"\n",
    "BATCH_SIZE = 256\n",
    "SRC_VOCAB_SIZE = len(tokenizer.src_wrap)\n",
    "TRG_VOCAB_SIZE = len(tokenizer.trg_wrap)\n",
    "ENCODER_EMBEDDING_DIM = 256\n",
    "DECODER_EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENCODER_DROPOUT = 0.5\n",
    "DECODER_DROPOUT = 0.5\n",
    "\n",
    "encoder = seq2seq_encoder(SRC_VOCAB_SIZE, ENCODER_EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, ENCODER_DROPOUT, bidirectional=True)\n",
    "\n",
    "decoder = seq2seq_decoder(TRG_VOCAB_SIZE, DECODER_EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DECODER_DROPOUT, attention=True,\n",
    "                          enc_hidden_dim=2*HIDDEN_DIM)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1996\n",
    "torch.manual_seed(seed)\n",
    "torch.mps.manual_seed(seed)\n",
    "\n",
    "def collate_func(batch):\n",
    "    src, trg = [], []\n",
    "    for b in batch:\n",
    "        src.append(b[0])\n",
    "        trg.append(b[1])\n",
    "    src_tokens, trg_tokens = tokenizer(src, trg)\n",
    "    src_batch, target_batch = torch.tensor( src_tokens , dtype=torch.long ) , torch.tensor( trg_tokens, dtype=torch.long  )\n",
    "\n",
    "    return src_batch, target_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_func)\n",
    "test_loader =  DataLoader(test_set, batch_size=BATCH_SIZE,  collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_id())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, data_loader, optimizer, device, teacher_ratio, clip=1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    optimizer.zero_grad()   \n",
    "    for src, trg in data_loader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        trg_out = trg[:, 1:]\n",
    "        logits = model(src, trg, teacher_ratio) \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(logits[:,1:,:].contiguous().view(-1, TRG_VOCAB_SIZE), trg_out.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "def evaluate_fn(model, data_loader, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in data_loader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            trg_input, trg_out = trg[:, :-1], trg[:, 1:]\n",
    "            logits = model(src, trg , 0) \n",
    "            loss = loss_fn(logits[:,1:,:].contiguous().view(-1, TRG_VOCAB_SIZE), trg_out.contiguous().view(-1))\n",
    "            epoch_loss += loss.item()           \n",
    "    return epoch_loss / len(data_loader)\n",
    "\n",
    "def translate_sentence( sentence:str, model:Seq2Seq, max_len = 25):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tensor = torch.tensor( tokenizer(sentence, \"\")[0] ).unsqueeze(0).to(DEVICE)\n",
    "        encoder_output, hidden, cell_state = model.encoder(tensor)\n",
    "        decoder_hidden = torch.zeros_like(hidden)  # Initialize decoder hidden state\n",
    "        decoder_cell = torch.zeros_like(hidden)\n",
    "        out_tokens = torch.ones(1, 1).fill_(tokenizer.sos_id()).type(torch.long).to(DEVICE)\n",
    "        for i in range(max_len):\n",
    "            output, decoder_hidden, decoder_cell  = model.decoder(out_tokens[:,i].unsqueeze(0), decoder_hidden, decoder_cell, encoder_output)\n",
    "            _, predicted_token = torch.max(torch.softmax(output, dim=-1), dim=1)\n",
    "            predicted_token = predicted_token[-1].item()\n",
    "            out_tokens = torch.cat([out_tokens, torch.ones(1, 1).type_as(out_tokens.data).fill_(predicted_token).to(DEVICE)], dim=1)\n",
    "            if predicted_token == tokenizer.eos_id():\n",
    "                break\n",
    "        return \"\".join( tokenizer.decode( out_tokens.cpu().tolist()[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_sentence(test_data.source[0], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS=20\n",
    "\n",
    "gc.collect()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "train_loss_list, valid_loss_list = [], []\n",
    "torch.mps.empty_cache()  \n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "best_model = copy.deepcopy(model)\n",
    "sentence = test_data.source[0]\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = timer()\n",
    "    train_loss = train_fn( model, train_loader, optimizer, DEVICE, teacher_ratio=0.5)\n",
    "    valid_loss = evaluate_fn( model, test_loader, DEVICE )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save(model.state_dict(), os.path.join(\"models\", \"checkpoint.pt\"))\n",
    "    end_time = timer()  \n",
    "    print(f\"Epoch: {epoch+1:02}\\t time = {(end_time - start_time):.3f}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")\n",
    "    print(f\"Original text: {sentence}\")\n",
    "    print(f\"Translated text: {translate_sentence(sentence, best_model)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, 'models/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_val = 0\n",
    "sentence = val_data.source[n_val]\n",
    "translation = translate_sentence(sentence, best_model)\n",
    "\n",
    "print(f\"Original text: {sentence}\")\n",
    "print(f\"Translated text: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_data.target[n_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
